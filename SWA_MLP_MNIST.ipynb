{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "import keras as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./tmp/data/\", one_hot=True) # 已经归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n",
      "(5000, 784) (5000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_valid, y_valid = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 展示几张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEICAYAAADm98d9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(mnist.train.labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(lr):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(784, input_shape=(784,), kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2)) \n",
    "\n",
    "    model.add(Dense(784, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2)) \n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(optimizer=K.optimizers.SGD(lr=lr),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 17s 302us/step - loss: 0.4361 - acc: 0.8825 - val_loss: 0.2476 - val_acc: 0.9302\n",
      "Epoch 2/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.2202 - acc: 0.9376 - val_loss: 0.1769 - val_acc: 0.9518\n",
      "Epoch 3/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.1684 - acc: 0.9521 - val_loss: 0.1462 - val_acc: 0.9602\n",
      "Epoch 4/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.1367 - acc: 0.9611 - val_loss: 0.1249 - val_acc: 0.9662\n",
      "Epoch 5/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 160us/step - loss: 0.1149 - acc: 0.9678 - val_loss: 0.1106 - val_acc: 0.9696\n",
      "Epoch 6/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.0982 - acc: 0.9723 - val_loss: 0.1020 - val_acc: 0.9714\n",
      "Epoch 7/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 172us/step - loss: 0.0851 - acc: 0.9761 - val_loss: 0.0940 - val_acc: 0.9738\n",
      "Epoch 8/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0746 - acc: 0.9790 - val_loss: 0.0914 - val_acc: 0.9734\n",
      "Epoch 9/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0654 - acc: 0.9821 - val_loss: 0.0845 - val_acc: 0.9760\n",
      "Epoch 10/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.0588 - acc: 0.9841 - val_loss: 0.0785 - val_acc: 0.9784\n",
      "Epoch 11/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.0523 - acc: 0.9863 - val_loss: 0.0794 - val_acc: 0.9770\n",
      "Epoch 12/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.0467 - acc: 0.9880 - val_loss: 0.0723 - val_acc: 0.9800\n",
      "Epoch 13/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 159us/step - loss: 0.0418 - acc: 0.9893 - val_loss: 0.0719 - val_acc: 0.9776\n",
      "Epoch 14/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 160us/step - loss: 0.0376 - acc: 0.9907 - val_loss: 0.0701 - val_acc: 0.9798\n",
      "Epoch 15/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 157us/step - loss: 0.0337 - acc: 0.9919 - val_loss: 0.0678 - val_acc: 0.9794\n",
      "Epoch 16/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0306 - acc: 0.9927 - val_loss: 0.0666 - val_acc: 0.9798\n",
      "Epoch 17/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.0271 - acc: 0.9943 - val_loss: 0.0652 - val_acc: 0.9798\n",
      "Epoch 18/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 173us/step - loss: 0.0244 - acc: 0.9949 - val_loss: 0.0642 - val_acc: 0.9820\n",
      "Epoch 19/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0632 - val_acc: 0.9812\n",
      "Epoch 20/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0200 - acc: 0.9963 - val_loss: 0.0628 - val_acc: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10b08940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from swa import LearningRateDisplay\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "lr = 0.05\n",
    "model_sgd = MLP(lr)\n",
    "lr_display_obj = LearningRateDisplay()\n",
    "model_sgd.fit(X_train, y_train, validation_data=(X_valid, y_valid),callbacks=[lr_display_obj], epochs=training_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 143us/step\n",
      "Test loss: 0.0648167538861\n",
      "Accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "loss, accuracy = model_sgd.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## swa\n",
    "三种学习率策略  \n",
    "![](./img/lr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from swa import SWA\n",
    "from lr_schedule import LR_schedule\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "swa_start = 17\n",
    "lr_start = 0.05\n",
    "lr_end = 0.04\n",
    "training_epochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 177us/step - loss: 0.4405 - acc: 0.8833 - val_loss: 0.2370 - val_acc: 0.9350\n",
      "Epoch 2/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 171us/step - loss: 0.2213 - acc: 0.9370 - val_loss: 0.1971 - val_acc: 0.9422\n",
      "Epoch 3/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.1714 - acc: 0.9513 - val_loss: 0.1492 - val_acc: 0.9574\n",
      "Epoch 4/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.1388 - acc: 0.9605 - val_loss: 0.1292 - val_acc: 0.9619\n",
      "Epoch 5/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 182us/step - loss: 0.1168 - acc: 0.9671 - val_loss: 0.1200 - val_acc: 0.9652\n",
      "Epoch 6/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.1000 - acc: 0.9716 - val_loss: 0.1127 - val_acc: 0.9657\n",
      "Epoch 7/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.0872 - acc: 0.9759 - val_loss: 0.0976 - val_acc: 0.9710\n",
      "Epoch 8/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0766 - acc: 0.9784 - val_loss: 0.0901 - val_acc: 0.9736\n",
      "Epoch 9/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0673 - acc: 0.9815 - val_loss: 0.0871 - val_acc: 0.9732\n",
      "Epoch 10/20\n",
      "learning rate of current epoch is : 0.049264706671237946\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0599 - acc: 0.9841 - val_loss: 0.0817 - val_acc: 0.9749\n",
      "Epoch 11/20\n",
      "learning rate of current epoch is : 0.04779411852359772\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0529 - acc: 0.9864 - val_loss: 0.0792 - val_acc: 0.9746\n",
      "Epoch 12/20\n",
      "learning rate of current epoch is : 0.04632353037595749\n",
      "55000/55000 [==============================] - 9s 171us/step - loss: 0.0473 - acc: 0.9875 - val_loss: 0.0793 - val_acc: 0.9753\n",
      "Epoch 13/20\n",
      "learning rate of current epoch is : 0.04485294222831726\n",
      "55000/55000 [==============================] - 10s 174us/step - loss: 0.0431 - acc: 0.9890 - val_loss: 0.0739 - val_acc: 0.9769\n",
      "Epoch 14/20\n",
      "learning rate of current epoch is : 0.04338235408067703\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.0390 - acc: 0.9902 - val_loss: 0.0731 - val_acc: 0.9777\n",
      "Epoch 15/20\n",
      "learning rate of current epoch is : 0.041911765933036804\n",
      "55000/55000 [==============================] - 10s 178us/step - loss: 0.0352 - acc: 0.9917 - val_loss: 0.0720 - val_acc: 0.9782\n",
      "Epoch 16/20\n",
      "learning rate of current epoch is : 0.040441177785396576\n",
      "55000/55000 [==============================] - 10s 175us/step - loss: 0.0321 - acc: 0.9925 - val_loss: 0.0733 - val_acc: 0.9775\n",
      "Epoch 17/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 10s 175us/step - loss: 0.0294 - acc: 0.9932 - val_loss: 0.0705 - val_acc: 0.9767\n",
      "Epoch 18/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 10s 180us/step - loss: 0.0270 - acc: 0.9945 - val_loss: 0.0703 - val_acc: 0.9776\n",
      "Epoch 19/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 12s 219us/step - loss: 0.0249 - acc: 0.9949 - val_loss: 0.0674 - val_acc: 0.9789\n",
      "Epoch 20/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 11s 192us/step - loss: 0.0232 - acc: 0.9956 - val_loss: 0.0696 - val_acc: 0.9786\n",
      "set stochastic weight average as final model parameters [FINISH].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x377da710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学习率下降 \n",
    "schedule = lambda epoch: LR_schedule(epoch, SWA_START=swa_start, lr_start=lr_start, lr_end=lr_end)\n",
    "lr_schedule_obj = LearningRateScheduler(schedule=schedule)\n",
    "swa_obj = SWA('',swa_start)\n",
    "model_swa1 = MLP(lr=lr_start)\n",
    "model_swa1.fit(X_train, y_train,callbacks=[lr_schedule_obj,swa_obj],validation_data=(X_test, y_test),epochs=training_epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 128us/step\n",
      "Test loss: 0.0675712019384\n",
      "Accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "loss, accuracy = model_swa1.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/20\n",
      "learning rate of current epoch is : 0.04899999871850014\n",
      "55000/55000 [==============================] - 10s 173us/step - loss: 0.4455 - acc: 0.8791 - val_loss: 0.2526 - val_acc: 0.9270\n",
      "Epoch 2/20\n",
      "learning rate of current epoch is : 0.04800000041723251\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.2240 - acc: 0.9359 - val_loss: 0.1836 - val_acc: 0.9467\n",
      "Epoch 3/20\n",
      "learning rate of current epoch is : 0.04699999839067459\n",
      "55000/55000 [==============================] - 9s 159us/step - loss: 0.1726 - acc: 0.9512 - val_loss: 0.1568 - val_acc: 0.9542\n",
      "Epoch 4/20\n",
      "learning rate of current epoch is : 0.04600000008940697\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.1397 - acc: 0.9607 - val_loss: 0.1312 - val_acc: 0.9613\n",
      "Epoch 5/20\n",
      "learning rate of current epoch is : 0.04500000178813934\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.1183 - acc: 0.9665 - val_loss: 0.1295 - val_acc: 0.9618\n",
      "Epoch 6/20\n",
      "learning rate of current epoch is : 0.04399999976158142\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.1017 - acc: 0.9715 - val_loss: 0.1060 - val_acc: 0.9678\n",
      "Epoch 7/20\n",
      "learning rate of current epoch is : 0.0430000014603138\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0890 - acc: 0.9751 - val_loss: 0.0984 - val_acc: 0.9699\n",
      "Epoch 8/20\n",
      "learning rate of current epoch is : 0.041999999433755875\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0791 - acc: 0.9777 - val_loss: 0.0940 - val_acc: 0.9719\n",
      "Epoch 9/20\n",
      "learning rate of current epoch is : 0.04100000113248825\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0706 - acc: 0.9806 - val_loss: 0.0867 - val_acc: 0.9727\n",
      "Epoch 10/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0636 - acc: 0.9825 - val_loss: 0.0820 - val_acc: 0.9755\n",
      "Epoch 11/20\n",
      "learning rate of current epoch is : 0.04899999871850014\n",
      "55000/55000 [==============================] - 10s 178us/step - loss: 0.0586 - acc: 0.9843 - val_loss: 0.0797 - val_acc: 0.9758\n",
      "Epoch 12/20\n",
      "learning rate of current epoch is : 0.04800000041723251\n",
      "55000/55000 [==============================] - 10s 177us/step - loss: 0.0523 - acc: 0.9861 - val_loss: 0.0789 - val_acc: 0.9758\n",
      "Epoch 13/20\n",
      "learning rate of current epoch is : 0.04699999839067459\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0470 - acc: 0.9879 - val_loss: 0.0742 - val_acc: 0.9774\n",
      "Epoch 14/20\n",
      "learning rate of current epoch is : 0.04600000008940697\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0423 - acc: 0.9893 - val_loss: 0.0739 - val_acc: 0.9767\n",
      "Epoch 15/20\n",
      "learning rate of current epoch is : 0.04500000178813934\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0382 - acc: 0.9903 - val_loss: 0.0713 - val_acc: 0.9782\n",
      "Epoch 16/20\n",
      "learning rate of current epoch is : 0.04399999976158142\n",
      "55000/55000 [==============================] - 9s 162us/step - loss: 0.0345 - acc: 0.9914 - val_loss: 0.0690 - val_acc: 0.9780\n",
      "Epoch 17/20\n",
      "learning rate of current epoch is : 0.0430000014603138\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0314 - acc: 0.9923 - val_loss: 0.0683 - val_acc: 0.9794\n",
      "Epoch 18/20\n",
      "learning rate of current epoch is : 0.041999999433755875\n",
      "55000/55000 [==============================] - 10s 174us/step - loss: 0.0286 - acc: 0.9935 - val_loss: 0.0703 - val_acc: 0.9788\n",
      "Epoch 19/20\n",
      "learning rate of current epoch is : 0.04100000113248825\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0260 - acc: 0.9946 - val_loss: 0.0667 - val_acc: 0.9803\n",
      "Epoch 20/20\n",
      "learning rate of current epoch is : 0.03999999910593033\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0238 - acc: 0.9951 - val_loss: 0.0664 - val_acc: 0.9798\n",
      "set stochastic weight average as final model parameters [FINISH].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3b03e710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 循环学习率\n",
    "schedule = lambda epoch: LR_schedule(epoch, flag=True,SWA_START=swa_start, lr_start=lr_start, lr_end=lr_end, c=10)\n",
    "lr_schedule_obj = LearningRateScheduler(schedule=schedule)\n",
    "swa_obj = SWA('',swa_start)\n",
    "model_swa2 = MLP(lr=lr_start)\n",
    "model_swa2.fit(X_train, y_train,callbacks=[lr_schedule_obj,swa_obj],validation_data=(X_test, y_test),epochs=training_epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 149us/step\n",
      "Test loss: 0.0651302281858\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "loss, accuracy = model_swa2.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 176us/step - loss: 0.4398 - acc: 0.8814 - val_loss: 0.2386 - val_acc: 0.9319\n",
      "Epoch 2/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.2214 - acc: 0.9367 - val_loss: 0.1796 - val_acc: 0.9471\n",
      "Epoch 3/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.1702 - acc: 0.9521 - val_loss: 0.1483 - val_acc: 0.9554\n",
      "Epoch 4/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.1378 - acc: 0.9609 - val_loss: 0.1290 - val_acc: 0.9617\n",
      "Epoch 5/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.1156 - acc: 0.9675 - val_loss: 0.1166 - val_acc: 0.9644\n",
      "Epoch 6/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0981 - acc: 0.9724 - val_loss: 0.1045 - val_acc: 0.9700\n",
      "Epoch 7/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.0856 - acc: 0.9759 - val_loss: 0.0943 - val_acc: 0.9703\n",
      "Epoch 8/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.0748 - acc: 0.9792 - val_loss: 0.0887 - val_acc: 0.9735\n",
      "Epoch 9/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.0661 - acc: 0.9817 - val_loss: 0.0878 - val_acc: 0.9733\n",
      "Epoch 10/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 172us/step - loss: 0.0589 - acc: 0.9839 - val_loss: 0.0776 - val_acc: 0.9769\n",
      "Epoch 11/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 173us/step - loss: 0.0521 - acc: 0.9858 - val_loss: 0.0781 - val_acc: 0.9750\n",
      "Epoch 12/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 9s 172us/step - loss: 0.0464 - acc: 0.9879 - val_loss: 0.0744 - val_acc: 0.9771\n",
      "Epoch 13/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 176us/step - loss: 0.0418 - acc: 0.9891 - val_loss: 0.0722 - val_acc: 0.9771\n",
      "Epoch 14/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 173us/step - loss: 0.0374 - acc: 0.9901 - val_loss: 0.0700 - val_acc: 0.9780\n",
      "Epoch 15/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 177us/step - loss: 0.0334 - acc: 0.9920 - val_loss: 0.0676 - val_acc: 0.9786\n",
      "Epoch 16/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 176us/step - loss: 0.0303 - acc: 0.9931 - val_loss: 0.0669 - val_acc: 0.9791\n",
      "Epoch 17/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 176us/step - loss: 0.0274 - acc: 0.9940 - val_loss: 0.0674 - val_acc: 0.9799\n",
      "Epoch 18/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 177us/step - loss: 0.0246 - acc: 0.9949 - val_loss: 0.0666 - val_acc: 0.9800\n",
      "Epoch 19/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 183us/step - loss: 0.0221 - acc: 0.9958 - val_loss: 0.0650 - val_acc: 0.9805\n",
      "Epoch 20/20\n",
      "learning rate of current epoch is : 0.05000000074505806\n",
      "55000/55000 [==============================] - 10s 174us/step - loss: 0.0202 - acc: 0.9961 - val_loss: 0.0636 - val_acc: 0.9801\n",
      "set stochastic weight average as final model parameters [FINISH].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x375d4dd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 恒定学习率\n",
    "swa_obj = SWA('',swa_start)\n",
    "model_swa3 = MLP(lr=lr_start)\n",
    "model_swa3.fit(X_train, y_train,callbacks=[swa_obj],validation_data=(X_test, y_test),epochs=training_epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 136us/step\n",
      "Test loss: 0.063436613702\n",
      "Accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "loss, accuracy = model_swa3.evaluate(mnist.test.images, mnist.test.labels)\n",
    "print('Test loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
